{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a758a2e",
   "metadata": {},
   "source": [
    "## Onboard Joint Research Center (JRC) data to OS-C S3\n",
    "\n",
    "The data is flood depth historical return period data and can be found at [JRC data catalog](https://data.jrc.ec.europa.eu/dataset/1d128b6c-a4ee-4858-9e34-6210707f3c81). The methodology is detailed at [\"A new dataset of river flood hazard maps for Europe and the Mediterranean Basin\" by  Francesco Dottori, Lorenzo Alfieri, Alessandra Bianchi, Jon Skoien, and Peter Salamon](https://essd.copernicus.org/articles/14/1549/2022/).\n",
    "\n",
    "The provide six different return periods: 10, 20, 50, 100, 200 and 500 years.\n",
    "\n",
    "The coordinates system of the map is EPSG:3035 (ETRS89-extended / LAEA Europe) and needs to be translated to Latitud-Longitud coordinate system. See EPSG official website for more information. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f1bc2a8",
   "metadata": {},
   "source": [
    "To guess the bound exactly instead of approximate we can use the Spanish bounds lat-lon coordinates for the peninsula and transform them to EPSG 25830. Then repeat for Canary Islands."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9e3d9d0",
   "metadata": {},
   "source": [
    "## Create Zarr from shape and Affine transformation\n",
    "\n",
    "<span style=\"color:blue\">Note: the file must be located in /hazard/src/ for the dependencies to work</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e42591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xarray\\backends\\cfgrib_.py:27: UserWarning: Failed to load cfgrib - most likely there is a problem accessing the ecCodes library. Try `import cfgrib` to get the full error message\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import s3fs\n",
    "import zarr\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import math\n",
    "import xarray as xr\n",
    "\n",
    "from pyproj.crs import CRS\n",
    "from affine import Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abc8b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazard.sources.osc_zarr import OscZarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac9bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://console-openshift-console.apps.odh-cl1.apps.os-climate.org/k8s/ns/sandbox/secrets/physrisk-s3-keys\n",
    "# default_staging_bucket = 'redhat-osc-physical-landing-647521352890'\n",
    "# OSC_S3_ACCESS_KEY, OSC_S3_SECRET_KEY\n",
    "\n",
    "# Hazard indicators bucket\n",
    "default_staging_bucket = \"physrisk-hazard-indicators\"\n",
    "prefix = \"hazard\"\n",
    "\n",
    "# Acess key and secret key are stored as env vars OSC_S3_HI_ACCESS_KEY and OSC_S3_HI_SECRET_KEY, resp.\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    anon=False,\n",
    "    key=os.environ[\"OSC_S3_HI_ACCESS_KEY\"],\n",
    "    secret=os.environ[\"OSC_S3_HI_SECRET_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bff5b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_path = os.path.join(\n",
    "    default_staging_bucket, prefix, \"riverflood_JRC_RP_hist.zarr\"\n",
    ").replace(\"\\\\\", \"/\")\n",
    "store = s3fs.S3Map(root=group_path, s3=s3, check=False)\n",
    "root = zarr.group(store=store, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9839520d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['physrisk-hazard-indicators/hazard/riverflood_JRC_RP010_hist.zarr',\n",
       " 'physrisk-hazard-indicators/hazard/riverflood_JRC_RP_hist.zarr']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.ls(\"physrisk-hazard-indicators/hazard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cad50aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the zarr file was created\n",
    "group_path in s3.ls(\"physrisk-hazard-indicators/hazard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09781e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "oscZ = OscZarr(bucket=default_staging_bucket, prefix=\"hazard\", s3=s3, store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17d36f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = r\"E:/JRC_RP/floodMap_RP{}/floodmap_EFAS_RP{}_C.tif\".format(\n",
    "    return_period, return_period\n",
    ")\n",
    "src = rasterio.open(path_to_file)\n",
    "\n",
    "transform = src.transform\n",
    "crs = CRS.from_epsg(3035)\n",
    "width = src.width\n",
    "height = src.height\n",
    "shape = (width, height)\n",
    "\n",
    "return_periods_str = [\"010\", \"020\", \"050\", \"100\", \"200\", \"500\"]\n",
    "return_periods = [int(rt) for rt in return_periods_str]\n",
    "\n",
    "src.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f439e73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zarr.core.Array '/physrisk-hazard-indicators/hazard/riverflood_JRC_RP_hist.zarr' (6, 63976, 45242) float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oscZ._zarr_create(\n",
    "    path=group_path,\n",
    "    shape=shape,\n",
    "    transform=transform,\n",
    "    crs=str(crs),\n",
    "    overwrite=True,\n",
    "    return_periods=return_periods,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a02aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create xr.DataArray from s3 stored zarr object\n",
    "\n",
    "# This will break arise memory error\n",
    "z = oscZ.root[group_path]\n",
    "da = xr.DataArray(data=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10222091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "# da = oscZ.read(path=group_path)\n",
    "# da\n",
    "\n",
    "# Return RuntimeError because of coords when creating Datarray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df124dca",
   "metadata": {},
   "source": [
    "## Steps to populate riverflood_JRC_RP_hist.zarr for 100m resolution\n",
    "\n",
    "### Step 1: Read JRC flood data\n",
    "\n",
    "Returns flood depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87a8f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_file(path_to_file):\n",
    "    \"\"\"\n",
    "    Read JRC data.\n",
    "\n",
    "    Parameters:\n",
    "        path_to_file (str): full path to tif file.\n",
    "\n",
    "    Returns:\n",
    "        fld_depth (numpy array): flood depth at (x1, y1) 3035 EPSG coordinates\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    src = rasterio.open(path_to_file)\n",
    "    fld_depth = src.read()\n",
    "\n",
    "    return fld_depth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d49d749",
   "metadata": {},
   "source": [
    "### Step 2: Populate the raster file for every return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac169636",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rt_i, rt in enumerate(return_periods_str):\n",
    "\n",
    "    path_to_file = r\"E:/JRC_RP/floodMap_RP{}/floodmap_EFAS_RP{}_C.tif\".format(rt, rt)\n",
    "    fld_depth = read_one_file(path_to_file)\n",
    "\n",
    "    da.data[rt_i, :, :] = fld_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcfc0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "oscZ.write(path=group_path, da=da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6043456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using root object. Better to use oscZ object\n",
    "\n",
    "\"\"\"\n",
    "create_dataset(name, **kwargs) method of zarr.hierarchy.Group instance\n",
    "    Create an array.\n",
    "    \n",
    "    Arrays are known as \"datasets\" in HDF5 terminology. For compatibility\n",
    "    with h5py, Zarr groups also implement the require_dataset() method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : string\n",
    "        Array name.\n",
    "    data : array-like, optional\n",
    "        Initial data.\n",
    "    shape : int or tuple of ints\n",
    "        Array shape.\n",
    "    chunks : int or tuple of ints, optional\n",
    "        Chunk shape. If not provided, will be guessed from `shape` and\n",
    "        `dtype`.\n",
    "    dtype : string or dtype, optional\n",
    "        NumPy dtype.\n",
    "    compressor : Codec, optional\n",
    "        Primary compressor.\n",
    "    fill_value : object\n",
    "        Default value to use for uninitialized portions of the array.\n",
    "\n",
    "\n",
    "\n",
    "root.create_dataset(name='prueba',\n",
    "                    data = np.array([[0,1], [1,6]]),\n",
    "                    shape = (2,2),\n",
    "                    chunks = (1000, 1000),\n",
    "                    dtype = 'f4')\n",
    "\n",
    "trans_members = [\n",
    "    transform.a,\n",
    "    transform.b,\n",
    "    transform.c,\n",
    "    transform.d,\n",
    "    transform.e,\n",
    "    transform.f,\n",
    "]\n",
    "mat3x3 = [x * 1.0 for x in trans_members] + [0.0, 0.0, 1.0] # Why adding this ??\n",
    "root.attrs[\"crs\"] = str(crs)\n",
    "root.attrs[\"transform_mat3x3\"] = mat3x3 \n",
    "if return_periods is not None:\n",
    "    root.attrs[\"index_values\"] = return_periods\n",
    "    root.attrs[\"index_name\"] = \"return period (years)\"\n",
    "\n",
    "# Read the file\n",
    "root['prueba']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to remove a file inside a bucket\n",
    "\n",
    "\"\"\"\"\n",
    "import boto3\n",
    "boto_c = boto3.client('s3', aws_access_key_id=os.environ[\"OSC_S3_ACCESS_KEY\"], aws_secret_access_key=os.environ[\"OSC_S3_SECRET_KEY\"])\n",
    "\n",
    "to_remove = boto_c.list_objects_v2(Bucket=default_staging_bucket, Prefix='hazard/hazard_MV_prueba.zarr')['Contents']\n",
    "\n",
    "keys = [item['Key'] for item in to_remove]\n",
    "\n",
    "for key_ in keys:\n",
    "    boto_c.delete_object(Bucket=default_staging_bucket, Key=key_)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
